{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H211VO45y3re"
      },
      "source": [
        "<small><font color=gray>Notebook author: <a href=\"https://www.linkedin.com/in/olegmelnikov/\" target=\"_blank\">Oleg Melnikov</a>\n",
        "\n",
        "**[<font size=6>üèÉHAR</font>](https://www.kaggle.com/competitions/29jan24hse-har/rules)**. [**Instructions**](https://colab.research.google.com/drive/1owkYjuRGkx050LQnM3b3yTzd0Dr2XbeV) for running Colabs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9WbQS1vzgf5"
      },
      "source": [
        "<small>**CONSENT.** <mark>[ X ]</mark> We consent to sharing our Colab (after the assignment ends) with other students/instructors for educational purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT3dcYSxxx_K",
        "outputId": "9abe4817-da2f-45aa-eac4-2d8a4ebb13c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')   # OK to enable, if your kaggle.json is stored in Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuKXMRjsyxoK",
        "outputId": "f801bf00-b5fc-4316-b0ce-e59b1a5509db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "- competition is now set to: 29jan24hse-har\n",
            "Using competition: 29jan24hse-har\n",
            "  teamId  teamName                submissionDate       score    \n",
            "--------  ----------------------  -------------------  -------  \n",
            "11629292  Z                       2024-02-17 20:30:41  0.96877  \n",
            "11633526  Maxim Smeyanov          2024-02-18 01:36:03  0.96809  \n",
            "11578655  Iskander Sergazin       2024-02-18 09:45:17  0.96673  \n",
            "11651220  AD                      2024-02-18 11:40:18  0.96401  \n",
            "11588638  Andrew Shatalov         2024-02-18 08:48:56  0.96401  \n",
            "11610716  R                       2024-02-18 10:48:09  0.96334  \n",
            "11633731  S                       2024-02-18 10:12:59  0.96334  \n",
            "11577989  Artem Zakharov          2024-02-18 08:54:04  0.96198  \n",
            "11579168  A                       2024-02-18 09:27:02  0.95994  \n",
            "11588231  G                       2024-02-13 12:10:37  0.95790  \n",
            "11594650  Mariia Sudakova         2024-02-05 07:34:38  0.95790  \n",
            "11591535  Artem Ilin              2024-02-03 14:46:22  0.95723  \n",
            "11591404  AF                      2024-02-03 07:23:58  0.95655  \n",
            "11619534  AJ                      2024-02-12 15:15:19  0.95519  \n",
            "11633671  Millena Cherepanova     2024-02-12 17:48:04  0.95315  \n",
            "11633717  AE                      2024-02-12 18:13:35  0.95315  \n",
            "11626825  X                       2024-02-18 10:38:33  0.95179  \n",
            "11596138  W                       2024-02-08 17:39:51  0.95112  \n",
            "11591981  AR                      2024-02-17 18:06:22  0.94976  \n",
            "11651999  Pavel Nedbay            2024-02-17 23:54:10  0.94908  \n",
            "11633549  Ilshat Dineev           2024-02-12 17:12:06  0.94840  \n",
            "11633785  Zapriagaeva Vlada       2024-02-12 18:49:35  0.94704  \n",
            "11633425  O                       2024-02-12 16:51:20  0.94636  \n",
            "11634188  P                       2024-02-12 20:58:27  0.94636  \n",
            "11634201  Mikhail Gorodov         2024-02-12 21:01:39  0.94636  \n",
            "11634265  Serebryakova Anastasia  2024-02-12 21:28:20  0.94636  \n",
            "11634112  RomaKholinov            2024-02-12 20:23:37  0.94501  \n",
            "11637719  Ivan Sinitsin           2024-02-13 18:18:49  0.94501  \n",
            "11637738  Andre Lebedinskiy       2024-02-13 18:23:46  0.94501  \n",
            "11595367  BAKUROV Timofei         2024-02-03 13:38:09  0.94433  \n",
            "11637803  Y                       2024-02-18 10:31:47  0.94433  \n",
            "11628999  Kseniia Nechaeva        2024-02-11 10:36:15  0.94297  \n",
            "11634212  Aristarkh Borshchev     2024-02-12 21:06:25  0.94093  \n",
            "11633010  Alina Dekkusheva        2024-02-12 14:36:26  0.94025  \n",
            "11633569  IlyaKozhevnikov         2024-02-12 17:26:00  0.93686  \n",
            "11633581  Naumenko Daria          2024-02-12 17:31:47  0.93211  \n",
            "11633038  AT                      2024-02-12 14:46:08  0.93075  \n",
            "11574845  Baseline                2024-01-29 19:58:15  0.92396  \n",
            "11631970  Denis Kulakov           2024-02-12 08:47:12  0.87508  \n"
          ]
        }
      ],
      "source": [
        "!pip -q install --upgrade --force-reinstall --no-deps kaggle > log  # upgrade kaggle package (to avoid a warning)\n",
        "!mkdir -p ~/.kaggle                                           # .kaggle folder must contain kaggle.json for kaggle executable to properly authenticate you to Kaggle.com\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json >log  # First, download kaggle.json from kaggle.com (in Account page) and place it in the root of mounted Google Drive\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json > log                   # Alternative location of kaggle.json (without a connection to Google Drive)\n",
        "!chmod 600 ~/.kaggle/kaggle.json                              # give only the owner full read/write access to kaggle.json\n",
        "!kaggle config set -n competition -v 29jan24hse-har           # set the competition context for the next few kaggle API calls. !kaggle config view - shows current settings\n",
        "!kaggle competitions download >> log                          # download competition dataset as a zip file\n",
        "!unzip -o *.zip >> log                                        # Kaggle dataset is copied as a single file and needs to be unzipped.\n",
        "!kaggle competitions leaderboard --show                       # print public leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lOhT2o18yxld",
        "outputId": "d93a1d06-7b58-433e-a9dd-f6c5d5375526"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 3.16 s, sys: 577 ms, total: 3.74 s\n",
            "Wall time: 5.52 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "%%capture\n",
        "%reset -f\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell as IS; IS.ast_node_interactivity = \"all\"\n",
        "import numpy as np, pandas as pd, time, os, random\n",
        "\n",
        "np.set_printoptions(linewidth=10000, precision=2, edgeitems=20, suppress=True)\n",
        "pd.set_option('display.max_colwidth', 1000, 'display.max_columns', 100, 'display.width', 1000, 'display.max_rows', 4)\n",
        "ToCSV = lambda df, fname: df.round(2).to_csv(f'{fname}.csv', index_label='id') # rounds values to 2 decimals\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchsummary import summary\n",
        "\n",
        "# Fallback options for TensorFlow + Keras and for SKLearn\n",
        "# import tensorflow as tf, tensorflow.keras as keras\n",
        "# from sklearn.neural_network import MLPClassifier   # SKLearn's MLP is optimised for CPU (and doesn't use GPU)\n",
        "# from keras.layers import Flatten, Dense\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "# os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "class Timer():\n",
        "  def __init__(self, lim:'RunTimeLimit'=60): self.t0, self.lim, _ = time.time(), lim, print(f'‚è≥ started. You have {lim} sec. Good luck!')\n",
        "  def ShowTime(self):\n",
        "    msg = f'Runtime is {time.time()-self.t0:.0f} sec'\n",
        "    print(f'\\033[91m\\033[1m' + msg + f' > {self.lim} sec limit!!!\\033[0m' if (time.time()-self.t0-1) > self.lim else msg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nELogoFQ5sRa",
        "outputId": "e63c5b19-fb4d-4608-8952-230a5c24d62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set as 0\n"
          ]
        }
      ],
      "source": [
        "# Always seed your experiments\n",
        "def set_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # When running on the CuDNN backend, two further options must be set\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    # Set a fixed value for the hash seed\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    print(f\"Random seed set as {seed}\")\n",
        "\n",
        "set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hZ7jNmp-LYNB",
        "outputId": "305539bb-b2c6-498d-ebcd-40c96770c9c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu activated\n"
          ]
        }
      ],
      "source": [
        "# Check if cuda activated.\n",
        "# If not, go to Runtime -> Change runtime type. Select 'T4 GPU'\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    print('cuda activated')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('cpu activated')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VA1oBPyyxii",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "0328fbc3-8a04-400a-9896-d11b14a56525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 468 ms, sys: 59.6 ms, total: 528 ms\n",
            "Wall time: 574 ms\n",
            "CPU times: user 1min 6s, sys: 8.77 s, total: 1min 15s\n",
            "Wall time: 1min 32s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        y       0       1       2       3       4       5       6       7       8       9      10      11      12      13      14      15      16      17      18      19      20      21      22      23      24      25      26      27      28      29      30      31      32      33      34      35      36      37      38      39      40      41      42      43     44      45      46      47      48  ...     511     512     513     514     515     516     517     518     519     520    521     522     523     524     525     526     527     528     529     530     531    532     533    534     535     536     537     538     539     540     541     542     543     544     545     546     547     548     549    550     551     552     553     554     555     556     557     558     559     560\n",
              "0       5  0.2778  0.0092 -0.0676 -0.9785 -0.9160 -0.9610 -0.9834 -0.9170 -0.9590 -0.9390 -0.4230 -0.7520  0.8496  0.6226  0.8400 -0.9434 -0.9614 -1.0370 -1.0150 -1.0070 -0.9640 -0.9550 -0.6772  0.0568  0.0192  0.5900 -0.3162  0.1833  0.4440 -0.2622  0.1092  0.4468 -0.4443 -0.1484  0.1718 -0.2727  0.0954 -0.4720 -0.5264  0.2332  0.9640 -0.1309  0.1071 -0.9814 -0.948 -0.9727 -0.9720 -0.9575 -0.9585  ... -0.9126 -0.2037 -0.5300 -0.8164 -0.9170 -0.8850 -0.9033 -0.9120 -0.9750 -0.9326 -1.014 -0.9560 -0.6780 -0.9966 -0.6180 -0.1021 -0.5977 -0.9546 -0.9110 -0.9260 -0.9297 -1.017 -0.9460 -1.022 -0.9570 -0.2930 -1.0100 -0.3455 -0.1411 -0.5215 -0.9585 -0.9160 -0.9434 -0.9414 -0.9750 -0.9414 -0.9890 -0.9610 -0.4453 -1.002 -0.5415 -0.0308 -0.5093  0.0380 -0.0912 -0.1415 -0.1316 -0.8200  0.1721 -0.0535\n",
              "1       1  0.2454  0.0073 -0.1046 -0.2010  0.1426 -0.2668 -0.2776  0.0648 -0.2605 -0.0572 -0.0364 -0.2830 -0.2830 -0.1448  0.4443 -0.0844 -0.6733 -0.7603 -0.7847 -0.4136 -0.3633 -0.1837  0.2830  0.5100  0.0582 -0.2502  0.3079 -0.1384  0.0822  0.0902 -0.0034  0.1969  0.0538  0.2996 -0.0258  0.0936 -0.3472 -0.1434 -0.4058  0.3690  0.9326 -0.2942 -0.0916 -0.9966 -0.964 -0.9663 -0.9746 -0.9736 -0.9634  ... -0.8115  0.4165 -0.4731 -0.8210  0.2542  0.2410  0.2688  0.0928 -0.7710  0.2430 -0.221 -0.1018  0.7134 -0.8994 -0.0642 -0.0842 -0.4750 -0.1345 -0.3853 -0.2573 -0.5430 -0.757 -0.1365 -0.677 -0.1826  0.6777 -0.7866  0.3240 -0.6206 -0.8530 -0.2500 -0.3025 -0.3176 -0.3198 -0.6426 -0.2488 -0.7236 -0.2512  0.6177 -0.910  0.1069 -0.0397 -0.4220  0.5480  0.6455  0.2296 -0.0335 -0.7000  0.2998  0.0880\n",
              "...    ..     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...    ...     ...     ...     ...     ...     ...     ...     ...     ...     ...     ...\n",
              "499998  4  0.2740 -0.0132 -0.1257 -0.9834 -1.0020 -0.9590 -0.9897 -0.9746 -0.9873 -0.9346 -0.5630 -0.8394  0.8306  0.6846  0.8350 -0.9840 -0.9824 -0.9960 -1.0200 -0.9950 -1.0160 -0.9590 -0.6500 -0.5225 -0.7974  0.5020 -0.2532  0.3723  0.1772  0.2920 -0.2756  0.3179 -0.1398  0.0948  0.0180 -0.1853  0.1871  0.0790 -0.0402 -0.0880  0.9785 -0.0442 -0.0532 -0.9950 -1.028 -0.9790 -1.0010 -0.9697 -1.0060  ... -0.2356  0.4312 -0.6030 -0.8706 -0.9700 -0.9863 -1.0010 -0.9990 -0.9927 -0.9910 -1.020 -1.0210 -0.9920 -0.9736  0.3857 -0.4620 -0.7485 -0.9985 -0.9575 -0.9897 -0.9814 -1.000 -0.9990 -1.008 -0.9660 -0.8574 -0.9210  0.1049 -0.6284 -0.8970 -1.0200 -1.0150 -0.9750 -1.0170 -0.9746 -0.9937 -0.9927 -0.9950 -1.0030 -0.844  0.2454 -0.3782 -0.7183 -0.0227  0.1957  0.1864  0.4556 -0.9326  0.1137  0.0595\n",
              "499999  5  0.2695 -0.0251 -0.1010 -1.0170 -0.9050 -0.9375 -0.9736 -0.8920 -0.9673 -0.9575 -0.5293 -0.8022  0.8530  0.6714  0.8480 -0.9624 -1.0205 -0.9900 -0.9600 -0.9960 -0.9480 -0.9720 -0.7320 -0.5117 -0.3535  0.3710 -0.2270  0.2700 -0.0636 -0.2438  0.0608  0.2050 -0.0218 -0.1199  0.0678  0.0154 -0.1132 -0.2886 -0.3882  0.6284  0.9966 -0.1277  0.0722 -1.0050 -0.925 -0.9440 -1.0050 -0.9824 -0.9233  ... -0.9500  0.0488 -0.3591 -0.7050 -1.0240 -0.9790 -0.9746 -0.9814 -0.9920 -0.9814 -1.013 -0.9860 -0.9650 -1.0150 -0.1430 -0.1555 -0.5180 -0.9320 -0.9200 -0.9424 -0.9326 -0.932 -0.9170 -0.985 -0.9463 -0.4020 -0.9640 -0.3160 -0.0948 -0.4695 -0.9590 -0.9500 -0.9976 -0.9680 -1.0340 -0.9727 -0.9900 -0.9790 -0.6980 -1.017 -0.4863  0.0084 -0.3293 -0.0127 -0.1399  0.4624 -0.7610 -0.8696  0.1720 -0.0272\n",
              "\n",
              "[500000 rows x 562 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2d5dff6-7121-4845-a8a5-f990fb25e81b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>...</th>\n",
              "      <th>511</th>\n",
              "      <th>512</th>\n",
              "      <th>513</th>\n",
              "      <th>514</th>\n",
              "      <th>515</th>\n",
              "      <th>516</th>\n",
              "      <th>517</th>\n",
              "      <th>518</th>\n",
              "      <th>519</th>\n",
              "      <th>520</th>\n",
              "      <th>521</th>\n",
              "      <th>522</th>\n",
              "      <th>523</th>\n",
              "      <th>524</th>\n",
              "      <th>525</th>\n",
              "      <th>526</th>\n",
              "      <th>527</th>\n",
              "      <th>528</th>\n",
              "      <th>529</th>\n",
              "      <th>530</th>\n",
              "      <th>531</th>\n",
              "      <th>532</th>\n",
              "      <th>533</th>\n",
              "      <th>534</th>\n",
              "      <th>535</th>\n",
              "      <th>536</th>\n",
              "      <th>537</th>\n",
              "      <th>538</th>\n",
              "      <th>539</th>\n",
              "      <th>540</th>\n",
              "      <th>541</th>\n",
              "      <th>542</th>\n",
              "      <th>543</th>\n",
              "      <th>544</th>\n",
              "      <th>545</th>\n",
              "      <th>546</th>\n",
              "      <th>547</th>\n",
              "      <th>548</th>\n",
              "      <th>549</th>\n",
              "      <th>550</th>\n",
              "      <th>551</th>\n",
              "      <th>552</th>\n",
              "      <th>553</th>\n",
              "      <th>554</th>\n",
              "      <th>555</th>\n",
              "      <th>556</th>\n",
              "      <th>557</th>\n",
              "      <th>558</th>\n",
              "      <th>559</th>\n",
              "      <th>560</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2778</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0676</td>\n",
              "      <td>-0.9785</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9390</td>\n",
              "      <td>-0.4230</td>\n",
              "      <td>-0.7520</td>\n",
              "      <td>0.8496</td>\n",
              "      <td>0.6226</td>\n",
              "      <td>0.8400</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9614</td>\n",
              "      <td>-1.0370</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-1.0070</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.9550</td>\n",
              "      <td>-0.6772</td>\n",
              "      <td>0.0568</td>\n",
              "      <td>0.0192</td>\n",
              "      <td>0.5900</td>\n",
              "      <td>-0.3162</td>\n",
              "      <td>0.1833</td>\n",
              "      <td>0.4440</td>\n",
              "      <td>-0.2622</td>\n",
              "      <td>0.1092</td>\n",
              "      <td>0.4468</td>\n",
              "      <td>-0.4443</td>\n",
              "      <td>-0.1484</td>\n",
              "      <td>0.1718</td>\n",
              "      <td>-0.2727</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>-0.4720</td>\n",
              "      <td>-0.5264</td>\n",
              "      <td>0.2332</td>\n",
              "      <td>0.9640</td>\n",
              "      <td>-0.1309</td>\n",
              "      <td>0.1071</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.948</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9126</td>\n",
              "      <td>-0.2037</td>\n",
              "      <td>-0.5300</td>\n",
              "      <td>-0.8164</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.8850</td>\n",
              "      <td>-0.9033</td>\n",
              "      <td>-0.9120</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-1.014</td>\n",
              "      <td>-0.9560</td>\n",
              "      <td>-0.6780</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.6180</td>\n",
              "      <td>-0.1021</td>\n",
              "      <td>-0.5977</td>\n",
              "      <td>-0.9546</td>\n",
              "      <td>-0.9110</td>\n",
              "      <td>-0.9260</td>\n",
              "      <td>-0.9297</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.9460</td>\n",
              "      <td>-1.022</td>\n",
              "      <td>-0.9570</td>\n",
              "      <td>-0.2930</td>\n",
              "      <td>-1.0100</td>\n",
              "      <td>-0.3455</td>\n",
              "      <td>-0.1411</td>\n",
              "      <td>-0.5215</td>\n",
              "      <td>-0.9585</td>\n",
              "      <td>-0.9160</td>\n",
              "      <td>-0.9434</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-0.9414</td>\n",
              "      <td>-0.9890</td>\n",
              "      <td>-0.9610</td>\n",
              "      <td>-0.4453</td>\n",
              "      <td>-1.002</td>\n",
              "      <td>-0.5415</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.5093</td>\n",
              "      <td>0.0380</td>\n",
              "      <td>-0.0912</td>\n",
              "      <td>-0.1415</td>\n",
              "      <td>-0.1316</td>\n",
              "      <td>-0.8200</td>\n",
              "      <td>0.1721</td>\n",
              "      <td>-0.0535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>-0.1046</td>\n",
              "      <td>-0.2010</td>\n",
              "      <td>0.1426</td>\n",
              "      <td>-0.2668</td>\n",
              "      <td>-0.2776</td>\n",
              "      <td>0.0648</td>\n",
              "      <td>-0.2605</td>\n",
              "      <td>-0.0572</td>\n",
              "      <td>-0.0364</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.2830</td>\n",
              "      <td>-0.1448</td>\n",
              "      <td>0.4443</td>\n",
              "      <td>-0.0844</td>\n",
              "      <td>-0.6733</td>\n",
              "      <td>-0.7603</td>\n",
              "      <td>-0.7847</td>\n",
              "      <td>-0.4136</td>\n",
              "      <td>-0.3633</td>\n",
              "      <td>-0.1837</td>\n",
              "      <td>0.2830</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>-0.2502</td>\n",
              "      <td>0.3079</td>\n",
              "      <td>-0.1384</td>\n",
              "      <td>0.0822</td>\n",
              "      <td>0.0902</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.1969</td>\n",
              "      <td>0.0538</td>\n",
              "      <td>0.2996</td>\n",
              "      <td>-0.0258</td>\n",
              "      <td>0.0936</td>\n",
              "      <td>-0.3472</td>\n",
              "      <td>-0.1434</td>\n",
              "      <td>-0.4058</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.9326</td>\n",
              "      <td>-0.2942</td>\n",
              "      <td>-0.0916</td>\n",
              "      <td>-0.9966</td>\n",
              "      <td>-0.964</td>\n",
              "      <td>-0.9663</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.9634</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.8115</td>\n",
              "      <td>0.4165</td>\n",
              "      <td>-0.4731</td>\n",
              "      <td>-0.8210</td>\n",
              "      <td>0.2542</td>\n",
              "      <td>0.2410</td>\n",
              "      <td>0.2688</td>\n",
              "      <td>0.0928</td>\n",
              "      <td>-0.7710</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>-0.221</td>\n",
              "      <td>-0.1018</td>\n",
              "      <td>0.7134</td>\n",
              "      <td>-0.8994</td>\n",
              "      <td>-0.0642</td>\n",
              "      <td>-0.0842</td>\n",
              "      <td>-0.4750</td>\n",
              "      <td>-0.1345</td>\n",
              "      <td>-0.3853</td>\n",
              "      <td>-0.2573</td>\n",
              "      <td>-0.5430</td>\n",
              "      <td>-0.757</td>\n",
              "      <td>-0.1365</td>\n",
              "      <td>-0.677</td>\n",
              "      <td>-0.1826</td>\n",
              "      <td>0.6777</td>\n",
              "      <td>-0.7866</td>\n",
              "      <td>0.3240</td>\n",
              "      <td>-0.6206</td>\n",
              "      <td>-0.8530</td>\n",
              "      <td>-0.2500</td>\n",
              "      <td>-0.3025</td>\n",
              "      <td>-0.3176</td>\n",
              "      <td>-0.3198</td>\n",
              "      <td>-0.6426</td>\n",
              "      <td>-0.2488</td>\n",
              "      <td>-0.7236</td>\n",
              "      <td>-0.2512</td>\n",
              "      <td>0.6177</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>0.1069</td>\n",
              "      <td>-0.0397</td>\n",
              "      <td>-0.4220</td>\n",
              "      <td>0.5480</td>\n",
              "      <td>0.6455</td>\n",
              "      <td>0.2296</td>\n",
              "      <td>-0.0335</td>\n",
              "      <td>-0.7000</td>\n",
              "      <td>0.2998</td>\n",
              "      <td>0.0880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499998</th>\n",
              "      <td>4</td>\n",
              "      <td>0.2740</td>\n",
              "      <td>-0.0132</td>\n",
              "      <td>-0.1257</td>\n",
              "      <td>-0.9834</td>\n",
              "      <td>-1.0020</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9873</td>\n",
              "      <td>-0.9346</td>\n",
              "      <td>-0.5630</td>\n",
              "      <td>-0.8394</td>\n",
              "      <td>0.8306</td>\n",
              "      <td>0.6846</td>\n",
              "      <td>0.8350</td>\n",
              "      <td>-0.9840</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0160</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.6500</td>\n",
              "      <td>-0.5225</td>\n",
              "      <td>-0.7974</td>\n",
              "      <td>0.5020</td>\n",
              "      <td>-0.2532</td>\n",
              "      <td>0.3723</td>\n",
              "      <td>0.1772</td>\n",
              "      <td>0.2920</td>\n",
              "      <td>-0.2756</td>\n",
              "      <td>0.3179</td>\n",
              "      <td>-0.1398</td>\n",
              "      <td>0.0948</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>-0.1853</td>\n",
              "      <td>0.1871</td>\n",
              "      <td>0.0790</td>\n",
              "      <td>-0.0402</td>\n",
              "      <td>-0.0880</td>\n",
              "      <td>0.9785</td>\n",
              "      <td>-0.0442</td>\n",
              "      <td>-0.0532</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.028</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9697</td>\n",
              "      <td>-1.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.2356</td>\n",
              "      <td>0.4312</td>\n",
              "      <td>-0.6030</td>\n",
              "      <td>-0.8706</td>\n",
              "      <td>-0.9700</td>\n",
              "      <td>-0.9863</td>\n",
              "      <td>-1.0010</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9910</td>\n",
              "      <td>-1.020</td>\n",
              "      <td>-1.0210</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>-0.4620</td>\n",
              "      <td>-0.7485</td>\n",
              "      <td>-0.9985</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.9897</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.000</td>\n",
              "      <td>-0.9990</td>\n",
              "      <td>-1.008</td>\n",
              "      <td>-0.9660</td>\n",
              "      <td>-0.8574</td>\n",
              "      <td>-0.9210</td>\n",
              "      <td>0.1049</td>\n",
              "      <td>-0.6284</td>\n",
              "      <td>-0.8970</td>\n",
              "      <td>-1.0200</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.9750</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9937</td>\n",
              "      <td>-0.9927</td>\n",
              "      <td>-0.9950</td>\n",
              "      <td>-1.0030</td>\n",
              "      <td>-0.844</td>\n",
              "      <td>0.2454</td>\n",
              "      <td>-0.3782</td>\n",
              "      <td>-0.7183</td>\n",
              "      <td>-0.0227</td>\n",
              "      <td>0.1957</td>\n",
              "      <td>0.1864</td>\n",
              "      <td>0.4556</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>0.1137</td>\n",
              "      <td>0.0595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499999</th>\n",
              "      <td>5</td>\n",
              "      <td>0.2695</td>\n",
              "      <td>-0.0251</td>\n",
              "      <td>-0.1010</td>\n",
              "      <td>-1.0170</td>\n",
              "      <td>-0.9050</td>\n",
              "      <td>-0.9375</td>\n",
              "      <td>-0.9736</td>\n",
              "      <td>-0.8920</td>\n",
              "      <td>-0.9673</td>\n",
              "      <td>-0.9575</td>\n",
              "      <td>-0.5293</td>\n",
              "      <td>-0.8022</td>\n",
              "      <td>0.8530</td>\n",
              "      <td>0.6714</td>\n",
              "      <td>0.8480</td>\n",
              "      <td>-0.9624</td>\n",
              "      <td>-1.0205</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9600</td>\n",
              "      <td>-0.9960</td>\n",
              "      <td>-0.9480</td>\n",
              "      <td>-0.9720</td>\n",
              "      <td>-0.7320</td>\n",
              "      <td>-0.5117</td>\n",
              "      <td>-0.3535</td>\n",
              "      <td>0.3710</td>\n",
              "      <td>-0.2270</td>\n",
              "      <td>0.2700</td>\n",
              "      <td>-0.0636</td>\n",
              "      <td>-0.2438</td>\n",
              "      <td>0.0608</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.1199</td>\n",
              "      <td>0.0678</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>-0.1132</td>\n",
              "      <td>-0.2886</td>\n",
              "      <td>-0.3882</td>\n",
              "      <td>0.6284</td>\n",
              "      <td>0.9966</td>\n",
              "      <td>-0.1277</td>\n",
              "      <td>0.0722</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.925</td>\n",
              "      <td>-0.9440</td>\n",
              "      <td>-1.0050</td>\n",
              "      <td>-0.9824</td>\n",
              "      <td>-0.9233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>0.0488</td>\n",
              "      <td>-0.3591</td>\n",
              "      <td>-0.7050</td>\n",
              "      <td>-1.0240</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.9746</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-0.9920</td>\n",
              "      <td>-0.9814</td>\n",
              "      <td>-1.013</td>\n",
              "      <td>-0.9860</td>\n",
              "      <td>-0.9650</td>\n",
              "      <td>-1.0150</td>\n",
              "      <td>-0.1430</td>\n",
              "      <td>-0.1555</td>\n",
              "      <td>-0.5180</td>\n",
              "      <td>-0.9320</td>\n",
              "      <td>-0.9200</td>\n",
              "      <td>-0.9424</td>\n",
              "      <td>-0.9326</td>\n",
              "      <td>-0.932</td>\n",
              "      <td>-0.9170</td>\n",
              "      <td>-0.985</td>\n",
              "      <td>-0.9463</td>\n",
              "      <td>-0.4020</td>\n",
              "      <td>-0.9640</td>\n",
              "      <td>-0.3160</td>\n",
              "      <td>-0.0948</td>\n",
              "      <td>-0.4695</td>\n",
              "      <td>-0.9590</td>\n",
              "      <td>-0.9500</td>\n",
              "      <td>-0.9976</td>\n",
              "      <td>-0.9680</td>\n",
              "      <td>-1.0340</td>\n",
              "      <td>-0.9727</td>\n",
              "      <td>-0.9900</td>\n",
              "      <td>-0.9790</td>\n",
              "      <td>-0.6980</td>\n",
              "      <td>-1.017</td>\n",
              "      <td>-0.4863</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.3293</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>-0.1399</td>\n",
              "      <td>0.4624</td>\n",
              "      <td>-0.7610</td>\n",
              "      <td>-0.8696</td>\n",
              "      <td>0.1720</td>\n",
              "      <td>-0.0272</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500000 rows √ó 562 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2d5dff6-7121-4845-a8a5-f990fb25e81b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c2d5dff6-7121-4845-a8a5-f990fb25e81b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c2d5dff6-7121-4845-a8a5-f990fb25e81b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-daccf9e6-59f6-4063-86a1-8d21a283c3dd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-daccf9e6-59f6-4063-86a1-8d21a283c3dd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-daccf9e6-59f6-4063-86a1-8d21a283c3dd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "tYX"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%time vX  = pd.read_csv('testX.csv', index_col='id')  # load testing input features X (only)\n",
        "%time tYX = pd.read_csv('trainYX.csv')                # partially load training labels Y and input features X\n",
        "tYX  # 561 input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ip8I5XHzYfZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "f0176844-9082-4425-871c-01187ebffce1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       5      1      3      4      2      6\n",
              "y  93667  83502  66901  87427  72554  95949"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5dc7dd02-0d03-43d1-a106-1598b4b93acc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>5</th>\n",
              "      <th>1</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>2</th>\n",
              "      <th>6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>y</th>\n",
              "      <td>93667</td>\n",
              "      <td>83502</td>\n",
              "      <td>66901</td>\n",
              "      <td>87427</td>\n",
              "      <td>72554</td>\n",
              "      <td>95949</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5dc7dd02-0d03-43d1-a106-1598b4b93acc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5dc7dd02-0d03-43d1-a106-1598b4b93acc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5dc7dd02-0d03-43d1-a106-1598b4b93acc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "cannot convert float NaN to integer"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tYX.y.value_counts(sort=False).to_frame().T  # counts of observations in each label category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1l4r_NEyzZ_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e00029b7-d8ae-4c2f-bec2-79ea395a6b7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ started. You have 60 sec. Good luck!\n"
          ]
        }
      ],
      "source": [
        "tmr = Timer() # runtime limit (in seconds). Add all of your code after the timer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl15K7562P-n"
      },
      "source": [
        "<hr color=red>\n",
        "\n",
        "<font size=5>‚è≥</font> <strong><font color=orange size=5>Your Code, Documentation, Ideas and Timer - All Start Here...</font></strong>\n",
        "\n",
        "Students: Keep all your definitions, code, documentation **between** ‚è≥ symbols."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpyLNt3god0c"
      },
      "source": [
        "## **Task 1. Preprocessing Pipeline**\n",
        "\n",
        "Explain elements of your preprocessing pipeline i.e. feature engineering, subsampling, clustering, dimensionality reduction, etc.\n",
        "1. Why did you choose these elements? (Something in EDA, prior experience,...? Btw, EDA is not required)\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30xYIFXAnaPE"
      },
      "source": [
        "**Student's answer:** For enhancing our model's accuracy we have tried the following functions:\n",
        "Feature Engineering and PCA: We attempted to enhance the dataset through polynomial feature engineering and dimensionality reduction via PCA. While these techniques can unveil intricate data relationships and reduce noise, they also risk discarding potentially useful information or overly simplifying the data structure, leading to a loss in model accuracy as it was as a result.\n",
        "Clustering to Identify Representative Observations: Using KMeans/KMedoid and FAISS for fast GPU-enabled clustering to reduce the datasete was one of the ideas. However, the selection of cluster centroids/medoids might not perfectly capture the diversity within the data significantly increase time and memory performance, that is why we were forced to steer clear from this idea.\n",
        "Stratified Sampling: Ensuring each label is proportionally represented aimed at improving model robustness and fairness. However, if not executed carefully, stratified sampling could introduce bias or overlook niche yet informative examples within smaller categories.\n",
        "\n",
        "Listed method were implemented but lead to fewer accuracy, so we decided to give them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGJRwzqHob4o"
      },
      "source": [
        "## **Task 2. Modeling Approach**\n",
        "Explain your modeling approach, i.e. ideas you tried and why you thought they would be helpful.\n",
        "\n",
        "1. How did these decisions guide you in modeling?\n",
        "1. How do you evaluate the effectiveness of these elements?\n",
        "1. What else have you tried that worked or didn't?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi6ZjgtWnb58"
      },
      "source": [
        "**Student's answer:**\n",
        "\n",
        "Firstly, we looked at the initial accuracy, which was pretty high, thus we thought that changing layers in Neural Network is not best starter idea. Thus, we looked at other parameters and saw that we can also change epoch, learning rate and momentum. These values also play a role in quality of NN model.\n",
        "*   Parameters:\n",
        "    *   learning rate: controls the step size\n",
        "\t*   momentum: controls the inertia of the optimizer\n",
        "    *   epoch: times running through the entire training set with a gradient descent algorithm\n",
        "*   Bad values can lead to:\n",
        "\t*   long training times\n",
        "\t*   bad overall performances (poor accuracy)\n",
        "\n",
        "Then we played with them a little bit, found best values for parameters and saw significant improve in accuracy.\n",
        "\n",
        "We then tried to play with NN layers, like adding one more, but it only significantly decreased the accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSKHErv_xQKA"
      },
      "outputs": [],
      "source": [
        "tX, tY = tYX.drop('y', axis=1).head(50000), tYX.head(50000).y-1   # shift labels by -1 to range {0,1,2,3,4,5}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ02ipVWbYsO"
      },
      "source": [
        "### NN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67E2C2GzaRFf"
      },
      "outputs": [],
      "source": [
        "# Define the PyTorch model\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 10)\n",
        "        self.fc2 = nn.Linear(10, 6)  # Assuming 6 is the number of classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Convert numpy arrays to torch tensors\n",
        "tX_tensor = torch.tensor(tX.values, dtype=torch.float32)\n",
        "tY_tensor = torch.tensor(tY.values, dtype=torch.long)\n",
        "\n",
        "# If using GPU\n",
        "tX_tensor = tX_tensor.to(device)\n",
        "tY_tensor = tY_tensor.to(device)\n",
        "\n",
        "# Create TensorDataset and split into train val sets\n",
        "dataset = TensorDataset(tX_tensor, tY_tensor)\n",
        "val_size = int(len(dataset) * 0.3)\n",
        "train_size = len(dataset) - val_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(input_size=tX.shape[1]).to(device)\n",
        "print(summary(model, input_size=(1, tX.shape[1])))\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.8)\n",
        "\n",
        "# Training loop\n",
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {np.round(running_loss/len(train_loader), 4)}\")\n",
        "\n",
        "    # Validation loop\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        print(f\"Validation Loss: {np.round(val_loss/len(val_loader), 4)}\")\n",
        "        print(f\"Accuracy: {np.round(100 * correct / total, 4)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8jgSQiPrBw_"
      },
      "outputs": [],
      "source": [
        "# Assuming vX is a numpy array containing your validation data\n",
        "# You need to convert it into a torch tensor with the correct type\n",
        "vX_tensor = torch.tensor(vX.values, dtype=torch.float32)\n",
        "\n",
        "# If using GPU, send the model and the tensor to GPU\n",
        "model.to('cuda')\n",
        "vX_tensor = vX_tensor.to('cuda')\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# No need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    # Make predictions\n",
        "    predictions = model(vX_tensor)\n",
        "\n",
        "# If you need class probabilities, apply softmax\n",
        "probabilities = torch.softmax(predictions, dim=1)\n",
        "\n",
        "# To get the predicted class labels, get the index of the max log-probability\n",
        "predicted_labels = torch.max(probabilities, 1)[1]\n",
        "\n",
        "# Convert to numpy array if needed (for further processing in non-PyTorch code)\n",
        "probabilities_np = probabilities.cpu().numpy()\n",
        "predicted_labels_np = predicted_labels.cpu().numpy()\n",
        "\n",
        "# Now 'probabilities_np' holds class probabilities and 'predicted_labels_np' holds class predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kDPxmFqaRNs"
      },
      "outputs": [],
      "source": [
        "YLab = [f'{i}/{s}' for i, s in enumerate('walking walking_upstairs walking_downstairs sitting standing laying'.split())]  # column labels\n",
        "pd.DataFrame(probabilities_np[:3,:], columns=YLab).style.background_gradient(cmap='coolwarm', axis=1)  # display first few predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG6rFvOyir0u"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(predicted_labels_np + 1, columns=['y']) # labels are shifted to the initial state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzGvrClxkB5y"
      },
      "outputs": [],
      "source": [
        "result.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXh8V5NbaRTI"
      },
      "outputs": [],
      "source": [
        "ToCSV(result, 'HAR_baseline') # generate a CSV submission file for Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzBsjCvS_kEw"
      },
      "source": [
        "# **References:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kr8Q-9T_nAb"
      },
      "source": [
        "1. https://numpy.org/doc/\n",
        "2. https://pytorch.org/docs/stable/index.html\n",
        "3. https://pypi.org/project/torch-summary/\n",
        "4. https://pandas.pydata.org/docs/\n",
        "5. https://ipython.readthedocs.io/en/stable/api/generated/IPython.core.interactiveshell.html\n",
        "6. https://app.datacamp.com/learn/courses/bayesian-data-analysis-in-python\n",
        "7. https://app.datacamp.com/learn/courses/introduction-to-deep-learning-with-pytorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoF2GoB_QGw9"
      },
      "source": [
        "<font size=5>‚åõ</font> <strong><font color=orange size=5>Do not exceed competition's runtime limit!</font></strong>\n",
        "\n",
        "<hr color=red>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nosV1OWFJPx5"
      },
      "outputs": [],
      "source": [
        "tmr.ShowTime()    # measure Colab's runtime. Do not remove. Keep as the last cell in your notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udpl5HJ4JSLr"
      },
      "source": [
        "# üí°**Starter Ideas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KOCCeay2r6G"
      },
      "source": [
        "1. Try tuning DNN hyperparameters\n",
        "1. Training set has 500K observations (2GB), but you really don't need them all. They are all bootstrapped (with noise) from the original sample of 7352 observations. In order to stay within Colab runtime limit (CRTL), you can\n",
        "  1. use more observations for a shallow DNN, but risk underfitting due to lower model complexity\n",
        "  1. use fewer observations for a deeper DNN, but risk overfitting to higher model complexity\n",
        "1. Check out the original related papers about feature engineering for this dataset\n",
        "1. Try engineering features with [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and discarding unimportant features via PCA or alternative technique.\n",
        "1. Consider KMeans/KMedoid or other clustering methods to identify observations, which represent the original 7352 observations. It might require finding 7352 cluster centroids/medoids.\n",
        "  1. Fast clustering methods: [FAISS](https://github.com/facebookresearch/faiss) (GPU-enabled)\n",
        "1. For deep NN, consider dropout, batch normalization\n",
        "1. Try PCA on transposed matrix to find/eliminate highly correlated observations\n",
        "1. Try [stratified sampling](https://en.wikipedia.org/wiki/Stratified_sampling) to ensure each label is proportionally represented in a subsample"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}